# -*- coding: utf-8 -*-
"""МОЙ_БЛОКНОТ_ДИПЛОМ.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ciYbTvuXyIl1g1g1AyoRAFB7sBi5yyXT

# Загрузка
"""

# Commented out IPython magic to ensure Python compatibility.
# обновить sqllite
'''
!wget https://www.sqlite.org/src/tarball/sqlite.tar.gz?r=release -O sqlite.tar.gz
!tar xzf sqlite.tar.gz
# %cd sqlite/
!./configure
!make sqlite3.c
# %cd /content
!npx degit coleifer/pysqlite3 -f
!cp sqlite/sqlite3.[ch] .
!python setup.py build_static build
!cp build/lib.linux-x86_64-3.6/pysqlite3/_sqlite3.cpython-36m-x86_64-linux-gnu.so \
    /usr/lib/python3.6/lib-dynload/_sqlite3.cpython-36m-x86_64-linux-gnu.so

# перезапустить среду выполнения
'''
import sqlite3
sqlite3.sqlite_version

"""Предполагая, что у вас есть триплеты на основе меток (т.е. они еще не преобразованы в числовые идентификаторы), вы можете использовать TriplesFactory.from_labeled_triples , примененные к df[["head", "relation", "tail"]].values(при условии, что ваши столбцы во фрейме данных называются «голова», «отношение» и «хвост»)."""

import pandas as pd
import numpy as np
import math
from itertools import zip_longest
from collections import defaultdict
import torch
import time

import matplotlib.pyplot as plt
from matplotlib import rc
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report
from collections import defaultdict
import pandas as pd
import numpy as np
import math
from itertools import zip_longest
from collections import defaultdict
import torch
import time
import torch
import torch.nn as nn
import seaborn as sns
from pylab import rcParams
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report
import warnings
warnings.filterwarnings("ignore")

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
#data.to(device)
device

"""## Марка - модель

### Marka
"""

# надо конечно ставить общую модель, но в коментах хохранять то что написал пользователь, чтобы если что видеть серию
# добавить а потом провевить как он определяет и какие номера реально 
Hy = ['hyundai','хёндать','хендать','хундать', 'хендая', 'нyundai','(хендай)','(hyundai)', 'lc-7','lc-v','lc-7', 'lc', '170w-7' , 'хюндать', 'lca', '210lc-7', '210-7', '170w-7',
      'r130lc-3','r130','r290l','r140',
     'r140w-7',  'r140lc-7',
     'r140lc-9',
      'r160lc-7','r160lc-7', 'r160',
      'r170w-7','r170w-7','r170w',
      'r170w-9',
      'r180lc-7',
      'r200w-7', 'r200w-7', 'r200w','r200w', 'r200w', 
      'r210w-9',
      'r210-7', 'r210','r210lc-7','r210-7' ,'r210','r210lc',
      'r210lc-9',
      'r220lc-9s',
      '220lc-v',
      'r250lc-7','r250lc-7','r250','r250lc-7a','r250-7',
       'r250lc-9',
        'r260lc-9s', 'r260lc-9s',
       'r290lc-7','r290-7','r290-7','r290lc-7','r290','r290lc-7a',
      'r290lc-9','r290lc-9',
      'r305lc-7',
      'r300lc-9s', 'r300',
       'r330lc-9s', 'r330lc-9',
        'r320lc-7', 'r320','r320-7','r320lc-7', 
       'r360lc-7', 'r360lc-7','r360', '360',
       'r380lc-9', '380',
    'r450lc-7', 'r450lc-7', 
    'r480lc-9',
    'r500lc-7', 'r500lc-7a',
    '500',
    '520',
    'r95w-3',]
marka_name_hyundai = ['Hyundai','Huyndai', 'Хёндай', 'Хендай','Хундай','Хенда']

Vo = ['volvo','blc','вольво', '(volvo)','(вольво)','bl','prime','волвый',
      'ec210','ec210b','ec210b','ec210','ec210blc','ес210',
      'ec240', 'ec240','ec240b',
      'bl71', 'bl71',
      'bl61', 'bl61','bl61b',
      'ec290',  'ec290', 'ec290b', 'ес290','ec290blc','ec290blc',
       'ec330',
      'ec360',  'ec360',
       '460','ec460', 
    'ec700','ec700b', ]

marka_name_volvo = ['Volvo','Вольво',]

Do = ['doosan','дасан','дусан','lc-a','(doosan)','solar','солар','салар', 'доосан', 'dl', 'dx', 'досан','дассан','sd300', 'daewoo','солара',  'солара', 
      'dx225','225lc-v','225lc-v','dx225','225nlc-v', 'dx225lca', '255lc-v',
      'dx255', 'dx255',
      'dx300lca',  'dx300lca','dx300',
      '340','340lc-v', 
       '420', '420lc-v','dl420a','dx420',
      'sd300n',
      'sd200n',]

marka_name_doosan = ['Doosan','Дасан','Дусан','Доосан', 'Досан','Дассан',]

"""### Model"""

# как сказать что 200 при комасту будет именно pc200  а не хендай 200
#if marka == 
# после анализа запросов добавить модели в выборки...как люди пишут...
# коматсу

# хендай
r130lc =['r130lc-3','r130','130']
r140w_7  =[ 'r140w-7', '140w 7']
r140w_9  =[ 'r140w-9', '140w 9']
r140lc_7  =['r140lc-7','r140lc 7','140lc-7''r140lc-7']
r140lc_9 =['r140lc-9','r140lc 9','140lc-9', 'r140lc-9',]
r160lc_7 =[ 'r160lc-7','r160lc-7', 'r160lc 7','160lc7',]
r160lc_9 =[ 'r160lc-9','r160lc-9','r160lc 9','160lc9',]

r170w_7 =[ 'r170w-7','р70w 7', '170w-7', '170w7']#'r170w'
r170w_9  =['r170w-9','170w-9', '170w9']
r180w_9  =[ 'r180w-9', 'r180w-9s', '180w 9','180в9', ]
r180lc_7=[ '180лс7',   'r180lc-7', '180lc-7']
r180lc_9=[ '180лс9',  'r180lc-9', '180lc-9']

r200w_7 =[     'r200w-7', 'r200w-7', 'r200w','r200w', 'r200w', ]
r210w_9 =[     'r210w-9','r210w-9s', '210w-9']
r210_7=[       'r210lc-7' ,'210lc-7', '210лс 7','р210лс7' ]
r210_9 =[     'r210lc-9','р210лс9','р210лс-9']
# общая для 210
r210  = ['r210','р210','р210лс', 'r210w','r210w', '210w','r210lc','210лс',] #  - не знаем префикс 7 или 9
r210_99 = ['r210 9','р210 9','р210 -9', 'r210-9','r210-9s', 'r210 9','210 9'] # не знаем lc, w для 9 серии
r210_77= ['r210-7','р210 7','р210-7', 'r210 7','210 7'] # не знаем lc, w для 7 серии

# общая для 250
r250  = ['r250','р250','р250лс', 'r250w','r250w', '250w','r250lc','250лс',] #  - не знаем префикс 7 или 9
r250_99 = ['r250 9','р250 9','р250 -9', 'r250-9','r250-9s', 'r250 9','250 9'] # не знаем lc, w для 9 серии
r250_77= ['r250-7','р250 7','р250-7', 'r250 7','250 7'] # не знаем lc, w для 7 серии


r220lc_9 =[     'r220lc-9s', '220lc-v','р220лс9','р220лс-9']
r250lc_7 =[     'r250lc-7','r250lc 7','r250lc-7a','р250-7','r250lc7','250lc-7','р250лс7','р250лс-7']#'r250',
r250lc_9  =[     'r250lc-9','r250lc 9','250lc-9','р250лс9','р250лс-9']
r260lc_9   =[     'r260lc-9s', 'r260lc-9s','r260lc 9','260lc-9','р260лс9','р260лс-9']
r290lc_7 =[    'r290lc-7','r290-7','r290-7','r290lc-7','r290','r290l','r290lc-7a',]
r290lc_9 =[     'r290lc-9','r290lc-9','r290lc 9','290lc-9','р290лс9','р290лс-9']
r305lc_7  =[    'r305lc-7','305','r305-7','r305 7','305lc-7','r305','р305']
r300lc_7   =[   '300lc-7','r300lc-7', '300lc 7']

r300lc_9   =[   'r300lc-9s', 'r300lc-9', 'r300lc 9' ]
r330lc_9  =[     'r330lc-9s', 'r330lc-9','р330', 'r330lc 9','р330-9']
r320lc_7  =[      'r320lc-7', 'r320','r320-7','r320lc-7', ]
r360lc_7   =[    'r360lc-7', 'r360lc-7','r360',]
r380lc_9   =[    'r380lc-9', 'r380lc-9sh','r380']
r450lc_7   =[ 'r450lc-7', 'r450lc-7', ]
r450lc_9  =[ 'r450lc-9', 'r450lc-9', ]

r480lc_9   =[ 'r480lc-9',]
r500lc_7   =[ 'r500lc-7', 'r500lc-7a', '500',]
r520lc   =[     '520',]
r95w   =[ 'r95w-3',]

model_hyundai =[r140lc_9, r140lc_7,r130lc,r140w_7,r140w_9,r160lc_7,r160lc_9,r170w_7,r170w_9,r180w_9,r180lc_7,\
                r180lc_9,r200w_7,r210w_9,r210_7,r210_9,r220lc_9,
    r250lc_7,r250lc_9,r260lc_9,r290lc_7,r290lc_9,r300lc_7,r305lc_7,r300lc_9,r330lc_9,r320lc_7,r360lc_7,r380lc_9,r450lc_7,
    r450lc_9, r480lc_9,r500lc_7,r520lc,r95w,
    r210,r210_99, r210_77, r250 ,r250_99, r250_77, ]
name_hyundai = ['r140lc-9', 'r140lc-7', 'r130lc','r140w-7','r140w-9','r160lc-7','r160lc-9','r170w-7','r170w-9',
                'r180w-9','r180lc-7','r180lc-9','r200w-7','r210w-9','r210lc-7','r210lc-9','r220lc-9',
'r250lc-7','r250lc-9','r260lc-9','r290lc-7','r290lc-9','r300lc-7','r305lc-7','r300lc-9','r330lc-9','r320lc-7','r360lc-7',
'r380lc-9','r450lc-7','r450lc-9','r480lc-9','r500lc-7','r520lc','r95w',
'210','r210/9', 'r210/7','250','r250/9', 'r250/7']


matrix_hyundai = [[model, a,'Hyundai',name_hyundai[i]] for model in marka_name_hyundai \
                  for i, value in enumerate(model_hyundai) for a in value]

# Volvo
ec180  =[    'ec180','ес180','ec180b','ec180b','ec180blc','ес180',]
ec140  =[    'ec140','ес140','ec140b','ec140b','ec140blc','ес140',]

ec210  =[    'ec210','ес210','ec210b','ec210b','ec210','ec210blc','ес210',]
ec240  =[     'ec240', 'ec240','ec240b', 'ec240blc',]
bl71   =[    'bl71', 'bl71',]
bl61   =[    'bl61', 'bl61','bl61b']
ec290  =[     'ec290',  'ec290', 'ec290b', 'ес290','ec290blc', 'ec290blc',]
ec330  =[      'ec330',]
ec360  =[     'ec360',  'ec360',]
ec380  =[     'ec380', ]

ec460  =[      '460','ec460', ]
ec700  =[ 'ec700','ec700b', ]

model_volvo=[ec140 , ec180, ec210,ec240,bl71,bl61,ec290,ec330,ec360,ec380,ec460,ec700]
name_volvo=['ec140' , 'ec180', 'ec210','ec240','bl71','bl61','ec290','ec330','ec360','ec380','ec460','ec700']
matrix_volvo = [[model, a,'Volvo',name_volvo[i]] for model in marka_name_volvo\
                  for i, value in enumerate(model_volvo) for a in value]

# Doosan
dx160 = ['DX160W','dx160w','dx160w',]
dx225 =[ 'dx225-lca', 'dx225lca','dx225 lca','dx225nlc' ]
dx255  =[      'dx255', 'dx255',]
dx300  =[ 'dx300',     'dx300lca',  'dx300lca','dx300lcv','dx300-lcv']
dx340    =[  '340lc-v']#, 'dx400','340']
dx360    =[ 'dx360']
dx420     =[    '420lc-v','dl420a','dx420',]# '420',
solar225nlcv = ['solar 225nlc-v','s225 nlc-v']
solar225lcv = ['s225lc-v','солар 225 лсв','solar 225 nlsv' , 'solar225 nlsv' ]
solar255lcv = ['s255lc-v','солар 255 lc-v',]
d225 = ['225лса','225lc','225лсв','225lcv','225 lcv','225lc-a','225lc-a']
d255 = ['255лса','255lc','255лсв','255lcv','255 lcv','255lc-a','255lc-a','255lc-v',]
sd300n    =[    'sd300n','sd300',]
sd200n      =[  'sd200n',]

model_doosan =[dx160,dx225,dx255,dx300,dx340,dx360,dx420,solar225nlcv,solar225lcv,solar255lcv,sd200n,sd300n,
               d225, d255]
name_doosan =['dx160','dx225','dx255','dx300','dx340','dx360','dx420','solar 225nlcv','solar 225lcv','solar 255lcv',
              'sd200','sd200', '225', '255']
matrix_doosan = [[model, a,'Doosan',name_doosan[i]] for model in marka_name_doosan\
                  for i, value in enumerate(model_doosan) for a in value]



model_number =[160,180,200,220,240,250,260,320,330,450, '210','220','330', '300','320', '290', '240','200', '160', '140', '225', '180', '260','250', '345','1200', '160w', '175', '325dl', '145',
 '255', '450', '130', '270', '108', '190', '375','320dl', '220-7', '480','600','902','100', '165', '135','170', '205','280']
name_number =[160,180,200,220,240,250,260,320,330,450, '210','220','330', '300','320', '290', '240','200', '160', '140', '225', '180', '260','250', '345','1200', '160w', '175', '325dl', '145',
 '255', '450', '130', '270', '108', '190', '375','320dl', '220-7', '480','600','902','100', '165', '135','170', '205','280']
all_frame_number = pd.DataFrame(columns=['model','name'])
all_frame_number['model'] = model_number
all_frame_number['name'] = name_number

import re
def cifra(text):
  
  r =re.compile("\d{3,4}")
  russian =re.findall(r,text)
  return russian

matrix_data =matrix_hyundai+matrix_doosan+matrix_volvo
all_frame_model = pd.DataFrame(matrix_data, columns=['marka_','model','marka','model_normal']).dropna()
#непомню зачем я получаю цифры из всего
all_frame_model['cifra'] = all_frame_model['model_normal'].apply(lambda x: ''.join(cifra(x)))
if __name__ == '__main__':
  all_frame_model.to_csv('/content/drive/MyDrive/учеба/ВЫШКА/ДИПЛОМ/Example/all_frame_model.csv')

print('Всего моделей - ',len(all_frame_model.model_normal.value_counts()),'Всего марок - ',len(all_frame_model.marka.value_counts()))
all_frame_model.head(5)

"""### Названия описания

### фигня
"""

'''
parts_mainpump = pd.read_csv('/content/drive/MyDrive/учеба/ВЫШКА/ДИПЛОМ/Example/parts_mainpump.csv' \
                      ,index_col=False)

dd = pd.DataFrame(prerare_exel(parts_mainpump) ,\
                   columns = ['name',	'marka',	'model',1,2,3])[['name','marka',	'model']].to_numpy()

df2 = pd.DataFrame(zapros_example_graph1(dd,20),columns = ['name',1,	'marka',	'model'])[['name','marka',	'model']]  #название - модель - префикс

df = pd.concat([df,df2])

df['marka_model'] = df['marka'] +'/'+ df['model']
'''

"""
ПЕределать

# создадим словарь где будут марки и регулярка для поиска номера. но все равно что делать просто с отдельными номерами типа 200 300 делать непонятно
slovar_search_numder = {'Hitachi' : [re.compile("\d{7}"), re.compile("[z,Z,E,e,е,E][x,X,х,Х]\d{3}[ ,-]?[3,5]?")],
                        'Komatsu': [re.compile("\w{3}-\w{2,3}-\w{4,5}") ,re.compile("[w,W][b,B]\d{2,3}[w,s]?[ ,-]?[5,6,7,2]?|[p,P,Р,р,p][c,C,с,С,c]\d{3,4}[ ,-]?[5,6,7,8]?|[d,D,д,Д][, ]?\d{2,3}")],
                        "Hyundai": [re.compile("\w{4}-\w{5}"),re.compile("[r,R]\d{3}[ ,-]?[w,s,W, ,l, L,л]?[c,C,с,С]?[ ,-]?[5,6,7,9]?[\w]?")],
                        "Cat": [re.compile("\w{3}-\w{4,5}"),"[d,D,д,Д]\d{1,2}[\w]?"],
                        "JCB" : [re.compile("\d{2,3}\s\d{5,6}"),re.compile("[3,4,5][ ,]?[c,с,С,C][\w]?")],
                        "Volvo" :[re.compile("voe\d{8}|\d{8}"),re.compile("[E,E,e,е][c,с,С,C][, ]?\d{3}[\w]?|[b,B,б,Б][l,L,л,Л][, ]?\d{2}[\w,\w]?")],
                        "Doosan": [re.compile("\d{3}-\d{5}|K\w{8}"),re.compile("[S,s,o,l,a,r]{5}[, ]?\d{3}[, ]?[,l, L,л]?[c,C,с,С]?[-]?\w?|[D,d,Д,д, ][x,X,х,Х,S,s]\d{3}[, ]?[,l, L,л]?[c,C,с,С]?[-]?\w?")]

                        }
name_mar_main = ['Hitachi','Komatsu','Hyundai','Cat','JCB','Volvo','Doosan']

Функция для выуживания марки и номера

# может переименовать????
def marka_number(df):
  all_index_oll = pd.DataFrame(columns = ['name','mm', 'nn',])
  #ttemp = pd.DataFrame(columns = ['mm', 'nn',])
  for i in name_mar_main:
    ttemp = pd.DataFrame(columns = ['name','mm', 'nn',])
    def model_search_v2(text):

      new_model =slovar_search_numder[i][1]
      number =slovar_search_numder[i][0]
      new_model_search =re.findall(new_model,text)
      number_search =re.findall(number,text)

      return ', '.join(new_model_search), ' '.join(number_search)
    ttemp['name'] = df.loc[df['marka'] == i]['name']
    ttemp['mm'],ttemp['nn'] = zip(*df.loc[df['marka'] == i]['name'].apply(model_search_v2))#.to_frame()
    all_index_oll =pd.concat ([all_index_oll, ttemp], axis = 0)

  return all_index_oll
"""
print()

import re
import numpy as np
'''
avito_parts = pd.read_csv('/content/drive/MyDrive/учеба/ВЫШКА/ДИПЛОМ/Example/avito.csv').dropna()

# вытащим номера и марки регулярками - из названия и описания
avito_parts['numder'] = avito_parts['!!!'].apply(number_found)
avito_parts['numder2'] = avito_parts['!!'].apply(number_found)
avito_parts['mod2'] = avito_parts['!!'].apply(model_found)
avito_parts['mod'] = avito_parts['!!!'].apply(model_found)
avito_parts.sort_values(by = 'mod', ascending=False).head(5)

avito_parts['mod_oll'] = func_dly_avito(avito_parts,'mod', 'mod2')
avito_parts['numder_oll'] = func_dly_avito(avito_parts,'numder', 'numder2')

#оставляем только те строки где есть марка и модель / не пустые
avito_parts_n = avito_parts.loc[avito_parts['mod_oll'].apply(lambda x: len(x)>0)].\
                  reset_index(drop=True)[['!!!' ,'Запчасти' ,'Модель','mod_oll','numder_oll']]

avito_parts_n.to_csv('/content/drive/MyDrive/учеба/ВЫШКА/ДИПЛОМ/Example/avito_m.csv')
'''

'''

nasos_gidrav_osnovnoi = pd.read_csv('/content/drive/MyDrive/учеба/ВЫШКА/ДИПЛОМ/Example/nasos_gidrav_osnovnoi.csv' ,\
                                    index_col=False).drop('Unnamed: 0', axis =1).dropna(subset=['name'])
nasos_gidrav_osnovnoi['numder'] = nasos_gidrav_osnovnoi['name'].apply(number_found).\
                                    apply(lambda x: ", ".join(str(i) for i in list(x)))

nasos_gidrav_osnovnoi['model_found'] = nasos_gidrav_osnovnoi['name'].apply(model_found).\
                                    apply(lambda x: ", ".join(str(i) for i in list(x)))

not_number = nasos_gidrav_osnovnoi.loc[nasos_gidrav_osnovnoi.numder.apply(lambda x: len(x)==0)]

not_number.query('model != "0" and marka != "0" and group2 != None')

not_number.fillna(0).query('model != "0" and marka != "0" and group1 == "Гидравлический"')

not_number.to_csv('/content/drive/MyDrive/учеба/ВЫШКА/ДИПЛОМ/Example/nasos_gidrav_osnovnoi_niot_number.csv')


'''

"""## Данные"""

spare_parts_name = {'Насос основной':['гидронасос', 'станция','гидростанция','насос' ,'main pump','гидравлический насос' ,
                                    'основной насос', 'насос основной','основной гидравлический насоc', 'главный насос',
                                     'гидронасос экскаватора','гидронасос на экскаватор','насос сдвоенный','гидронасос сдвоенный'], 
              
              'Гидромотор хода':['гидромотор хода','Г/мотор хода','мотор хода',
                                  'гидромотор к экскаватору для ходовой части','гидро мотор хода'],
              
              'Гидромотор поворота':['гидромотор поворота','Г/мотор поворота','мотор поворота'],

              'Редуктор хода':['редуктор хода','бортовая', 'бортовой редуктор','редуктор хода без мотора',
                                  'редуктор хода без гидромотора','редуктор хода без г/мотора','ходовой редуктор',
                                    'бортовой редуктор без гидромотора','редуктор хода бортовой','ходовой редуктор',
                                ],
              
              'Редуктор поворота':[ 'редуктор поворота', 'поворотный редуктор','редуктор поворота без г/мотора',
                                    'редуктор поворота без гидромотора','система поворота платформы',
                                    'механизм поворота','механизм поворота платформы','редуктор поворота башни',
                                    'редуктор механизма поворота', 'поворотный редуктор экскаватора',
                                    'поворотный мотор редуктор','купить поворотный редуктор на экскаватор',
                                    'поворотная платформа редуктора поворота','редуктора поворота башни '],
              
              'Редуктор хода в сборе' :[ 'редуктор в сборе', 'редуктор с мотором','бортовая с мотором',
                                          'бортовая в сборе', 'бортовой редуктор с мотором','редуктор хода в сборе',
                                          'редуктор хода в сборе с гидромотором', 'редуктор хода в сборе с г/мотором',
                                          'редуктор хода с гидромотором','редуктор хода с мотором',
                                          'редуктор хода в сборе с корпусом г/мотора', 'бортовой редуктор в сборе',
                                          'ходовой редуктор в сборе','ходовой редуктор с мотором',
                                        'бортовой редуктор хода в сборе с гидромотором','редуктор хода с гидромотором']
                    }

marka_mame = {'Hyundai' :['Hyundai','Huyndai', 'Хёндай', 'Хендай','Хундай','Хенда'],
              'Volvo' :['Volvo','Вольво',],
              'Doosan':['Doosan','Дасан','Дусан','Доосан', 'Досан','Дассан',]
            }

'''
Создадим тренировочные данные - полные,
впоследствии на основании этих данных мы сделаем тестовые примеры
'''
spare_parts_example =  [['Насос основной','Doosan', 'dx300', 'K1006550','100'],['Насос основной','Doosan', 'dx300', '401-00432','100'],
                        #['Насос основной','Doosan', 'dx300', '400914-00393','100'],['Насос основной','Doosan', 'dx300', '31Q8-10030','100'],
                        #['Насос основной','Hyundai', 'r300lc-9', '31Q8-10030','100'],['Насос основной','Hyundai', 'r300lc-9', '31Q8-10010','100'],
                        ['Насос основной','Doosan', 'dx300', '31Q8-10010','100'],['Насос основной','Hyundai', 'r300lc-9', '401-00432','100'],
                        ['Насос основной','Hyundai', 'r300lc-9', 'K1006550','100'],['Насос основной','Hyundai', 'r300lc-9', '400914-00393','100'],

                      #  ['Насос ','Hyundai', 'r300lc-9', 'K1006550','100'],['Насос основной','Hyundai', 'r300lc-9', '400914-00393','100'],
                        
                        ['Редуктор хода в сборе','Doosan', 'dx225', '170401-00039','150'], ['Редуктор хода в сборе','Doosan', 'dx225', '14575732','150'],
                        ['Редуктор хода в сборе','Doosan', 'dx225', '31N6-40050','150'], ['Редуктор хода в сборе','Hyundai', 'r210lc-7', '31N6-40050','150'],
                        ['Редуктор хода в сборе','Hyundai', 'r210lc-7', '31N6-40051','150'],['Редуктор хода в сборе','Volvo', 'ec210', 'K1037757','150'],
                        ['Редуктор хода в сборе','Volvo', 'ec210', '170401-00039','150'],['Редуктор хода в сборе','Volvo', 'ec240', '14575732','150'],
                        #['Редуктор хода в сборе','Doosan', 'dx225', 'K1037757','150'], ['Редуктор хода в сборе','Volvo', 'ec210', '14575732','150'],
                        #['Редуктор хода в сборе','Doosan', 'dx225', '31N6-40051','150'], ['Редуктор хода в сборе','Hyundai', 'r210lc-7', '14575732','150'],
                        #['Редуктор хода в сборе','Hyundai', 'r210lc-7', 'K1037757','150'],['Редуктор хода в сборе','Volvo', 'ec210', '31N6-40050','150'],
                        ['Редуктор хода в сборе','Volvo', 'ec240', 'K1037757','150'],['Редуктор хода в сборе','Volvo', 'ec240', '170401-00039','150'],
                        ['Редуктор хода в сборе','Hyundai', 'r210lc-7', '170401-00039','150'],['Редуктор хода в сборе','Volvo', 'ec210', '31N6-40051','150'],
                        #['Редуктор хода в сборе','Volvo', 'ec240', '31N6-40050','150'],['Редуктор хода в сборе','Volvo', 'ec240', '31N6-40051','150'],

                        ['Редуктор поворота','Volvo', 'ec240', '14542163','420'],
                        ['Редуктор поворота','Volvo', 'ec240', '7118-34100','420'],
                        ['Редуктор поворота','Volvo', 'ec240', '14566202','420'],

                        ['Редуктор поворота','Hyundai', 'r210lc-7', '31N6‑10180','400'],['Редуктор поворота','Hyundai', 'r170w-7', '31N6‑10180','400'],
                        ['Редуктор поворота','Hyundai', 'r180lc-7', '31N6‑10180','400'],['Редуктор поворота','Hyundai', 'r160lc-7', '31N6‑10180','400'],
                        #['Редуктор поворота','Hyundai', 'r210lc-7', '31N6‑10181','400'],['Редуктор поворота','Hyundai', 'r170w-7', '31N6‑10181','400'],
                        ['Редуктор поворота','Hyundai', 'r180lc-7', '31N6‑10181','400'],['Редуктор поворота','Hyundai', 'r160lc-7', '31N6‑10181','400'],
                        ['Редуктор поворота','Hyundai', 'r210lc-7', '31N6‑10150','400'],['Редуктор поворота','Hyundai', 'r170w-7', '31N6‑10150','400'],
                        #['Редуктор поворота','Hyundai', 'r180lc-7', '31N6‑10150','400'],['Редуктор поворота','Hyundai', 'r160lc-7', '31N6‑10150','400'],
                        ]

spare_parts_example_test = [
                        ['Редуктор поворота','Hyundai', 'r210lc-7', '31N6‑10181','400'],['Редуктор поворота','Hyundai', 'r170w-7', '31N6‑10181','400'],
                        ['Редуктор поворота','Hyundai', 'r180lc-7', '31N6‑10150','400'],['Редуктор поворота','Hyundai', 'r160lc-7', '31N6‑10150','400'],
                        ['Редуктор хода в сборе','Volvo', 'ec240', '31N6-40050','150'],['Редуктор хода в сборе','Volvo', 'ec240', '31N6-40051','150'],
                        ['Редуктор хода в сборе','Doosan', 'dx225', 'K1037757','150'], ['Редуктор хода в сборе','Volvo', 'ec210', '14575732','150'],
                        ['Редуктор хода в сборе','Doosan', 'dx225', '31N6-40051','150'], ['Редуктор хода в сборе','Hyundai', 'r210lc-7', '14575732','150'],
                        ['Редуктор хода в сборе','Hyundai', 'r210lc-7', 'K1037757','150'],['Редуктор хода в сборе','Volvo', 'ec210', '31N6-40050','150'],
                        ['Насос основной','Doosan', 'dx300', '400914-00393','100'],['Насос основной','Doosan', 'dx300', '31Q8-10030','100'],
                        ['Насос основной','Hyundai', 'r300lc-9', '31Q8-10030','100'],['Насос основной','Hyundai', 'r300lc-9', '31Q8-10010','100'],
                          ]

"""### Для базы данных"""

# еще данных
parts_mainpump = pd.read_csv(fixpath('/content/drive/MyDrive/учеба/ВЫШКА/ДИПЛОМ/Example/parts_mainpump.csv') ,index_col=False)
parts_mainpump[8:10]

baza_name_marke_model = pd.read_csv(fixpath('/content/drive/MyDrive/учеба/ВЫШКА/ДИПЛОМ/Example/all_frame_model.csv') ,index_col=False).drop('Unnamed: 0', axis =1)

# заберем только то что есть выборках
sp_model =list(set([row[2] for row in spare_parts_example]+ \
        [''.join(i.strip()) for row in parts_mainpump['модель'] for i in row.split(',')]
      ))

baza_name_marke_model = baza_name_marke_model.loc[baza_name_marke_model.query('model_normal in @sp_model').index].\
                                      reset_index(drop=True)

print('Всего моделей - ',len(baza_name_marke_model.model_normal.value_counts()),\
      'Всего марок - ',len(baza_name_marke_model.marka.value_counts()))

baza_name_marke_model[:3]

try:
  len(sp_model) == len(baza_name_marke_model.model_normal.value_counts())
except:
  print('Не хватает данных')

"""## Функции"""

# Commented out IPython magic to ensure Python compatibility.
def image_graph(history):
#   %config InlineBackend.figure_format='retina'
  sns.set(style='whitegrid', palette='muted', font_scale=1.2)
  HAPPY_COLORS_PALETTE = ["#01BEFE", "#FFDD00", "#FF7D00", "#FF006D", "#ADFF02", "#8F00FF"]
  sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))
  rcParams['figure.figsize'] = 12, 8

  plt.plot([i.cpu() for i in history['train_acc']], label='train accuracy')
  plt.plot([i.cpu() for i in history['val_acc']], label='validation accuracy')
  #plt.plot(history['val_acc'].cpu(), label='validation accuracy')

  plt.title('Training history')
  plt.ylabel('Accuracy')
  plt.xlabel('Epoch')
  plt.legend()
  plt.ylim([0, 1]);

def show_confusion_matrix(confusion_matrix):
  hmap = sns.heatmap(confusion_matrix, annot=True, fmt="d", cmap="Blues")
  hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')
  hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')
  plt.ylabel('True sentiment')
  plt.xlabel('Predicted sentiment');

def confusion_matrix_print(df, name,y_test, y_pred):
  '''
  Рисуем матрицу для конкретного столбца предсказаний
  '''
  new_dict_name = {value: key for key, value in enumerate(df[name].unique())}
  cm = confusion_matrix(y_test, y_pred, labels=list(new_dict_name.values()))
  df_cm = pd.DataFrame(cm, index=new_dict_name, columns=new_dict_name)
  show_confusion_matrix(df_cm)

def make_me_small(df):
  '''
  Обрезаем датасет по минимальному количеству элементов, чтобы у всех было одинаково
  '''
  minimum = df.drop_duplicates().marka_model.value_counts().min()
  a = []
  for i in df.marka_model.value_counts().index.to_list():
    ss = df.loc[df['marka_model']== i][:minimum].to_numpy()
    for b in ss:
      a.append(b)
  return pd.DataFrame(a, columns = ['name', 'marka', 'model','marka_model'])

def prepare_df_func(number,
                    model_do_you_need = True,
                    oll_model_do_you_need =[210, 250, 225],  \
                    start_way ='/content/drive/MyDrive/учеба/ВЫШКА/ДИПЛОМ/Example/all_frame_model.csv', \
                    count_out_model = 9,
                    rovno = False
                    ):
  '''
  Функция для подготовки датасета для обучения моделей
  start_way = путь до датасета с описаниями марок и моделей
  number - количество групп
  model_do_you_need - если нужно выбрать определенные модели
  oll_model_do_you_need - цифры тех модели которые нужны в выборке ([210, 250, 225]) - самые проблемные
  count_out_model - общее количество моделей( и подмоделей) которые мы получим на выходе
  rovno - нужно ли делать одинаковое количество всем моделей
  '''
  baza_name_marke_model = pd.read_csv(start_way ,index_col=False).drop('Unnamed: 0', axis =1)
  if model_do_you_need:
    baza_name_marke_model = baza_name_marke_model[baza_name_marke_model['cifra'].isin(oll_model_do_you_need)]

  spare_parts_name = {'Насос основной':['гидронасос', 'станция','гидростанция','насос' ,'main pump','гидравлический насос' ,
                                    'основной насос', 'насос основной','основной гидравлический насоc', 'главный насос',
                                     'гидронасос экскаватора','гидронасос на экскаватор',
                                      'насос сдвоенный','гидронасос сдвоенный'], 
                    
             
              'Гидромотор хода':['гидромотор хода','Г/мотор хода','мотор хода',
                                  'гидромотор к экскаватору для ходовой части','гидро мотор хода'],
              
              'Гидромотор поворота':['гидромотор поворота','Г/мотор поворота','мотор поворота'],

              'Редуктор хода':['редуктор хода','бортовая', 'бортовой редуктор','редуктор хода без мотора',
                                  'редуктор хода без гидромотора','редуктор хода без г/мотора','ходовой редуктор',
                                    'бортовой редуктор без гидромотора','редуктор хода бортовой','ходовой редуктор',
                                ],


              'Редуктор поворота':[ 'редуктор поворота', 'поворотный редуктор','редуктор поворота без г/мотора',
                                    'редуктор поворота без гидромотора','система поворота платформы',
                                    'механизм поворота','механизм поворота платформы','редуктор поворота башни',
                                    'редуктор механизма поворота', 'поворотный редуктор экскаватора',
                                    'поворотный мотор редуктор','купить поворотный редуктор на экскаватор',
                                    'поворотная платформа редуктора поворота','редуктора поворота башни '],
              
              'Редуктор хода в сборе' :[ 'редуктор в сборе', 'редуктор с мотором','бортовая с мотором',
                                          'бортовая в сборе', 'бортовой редуктор с мотором','редуктор хода в сборе',
                                          'редуктор хода в сборе с гидромотором', 'редуктор хода в сборе с г/мотором',
                                          'редуктор хода с гидромотором','редуктор хода с мотором',
                                          'редуктор хода в сборе с корпусом г/мотора', 'бортовой редуктор в сборе',
                                          'ходовой редуктор в сборе','ходовой редуктор с мотором',
                                        'бортовой редуктор хода в сборе с гидромотором','редуктор хода с гидромотором']
                    }

  test_spare_parts= {'Насос основной':['насос на экскаватор', 'экскаваторный насос',]}
  marka_mame = {'Hyundai' :['Hyundai','Huyndai', 'Хёндай', 'Хендай','Хундай','Хенда'],
                'Volvo' :['Volvo','Вольво',],
                'Doosan':['Doosan','Дасан','Дусан','Доосан', 'Досан','Дассан',]
            }

  if len(list(spare_parts_name.keys())) < number:
    number == len(list(spare_parts_name.keys()))
  
  # получаем наш датасет
  df = tare_me_oll_parts(spare_parts_name, baza_name_marke_model,number, longer = False)

  #какоче количество исходящих сочетаний марок/моделей нам нужно
  if len(df.marka_model.value_counts()) < count_out_model:
    count_out_model = len(df.marka_model.value_counts())
    list_model = list(df.marka_model.value_counts().index)[:count_out_model]
  else:
    list_model = list(df.marka_model.value_counts().index)[:count_out_model]

  df = df[df['marka_model'].isin(list_model)]

  class_names = {value: key for key, value in enumerate(df['marka_model'].unique())}
  df['marka_model_bert'] = df['marka_model'].map(class_names)

  if rovno:
    df = make_me_small(df)

  return class_names, df

def tare_me_oll_parts(spare_parts_name, baza_name_marke_model,number, longer = False):
  '''
  Функция которая готовит нашу базу для обучения
  spare_parts_name - словарь {групппа - варианты синонимов запчастей}
  baza_name_marke_model - df - [marka_,	model,	marka,	model_normal,	cifra]
  longer - брать больше с названиями запчастей (усложняет работу модели) или просто марка модель
  '''

  baza_name_marke_model = baza_name_marke_model.dropna()
  spisok_zaprosov = []
  dict_keys = spare_parts_name.keys()
  for name in dict_keys: # по всем ключам в названиях групп
    for diffferent_name in spare_parts_name[name]: #по всем названиям в одной группе
      for row in baza_name_marke_model.to_numpy(): # по всем номерам из базы номеров
        marka_of_parts = row[0]
        model_of_parts = row[1]
        model_of_parts_cifra = str(int(row[4]))
        marka_of_parts_normal = row[2]
        model_of_parts_normal = row[3]

        spisok1 = [diffferent_name.lower() + ' ' + marka_of_parts + ' ' + model_of_parts]+ [marka_of_parts_normal, model_of_parts_normal]
        if marka_of_parts_normal == 'Volvo':
          spisok2 = [diffferent_name + ' ' + marka_of_parts + ' ' + model_of_parts_cifra]+ [marka_of_parts_normal, model_of_parts_normal]
          spisok5 = [marka_of_parts + ' ' + model_of_parts_cifra +' ' +diffferent_name]+ [marka_of_parts_normal, model_of_parts_normal]
        else:
          spisok2 = [diffferent_name + ' ' + marka_of_parts + ' ' + model_of_parts_cifra]+ [marka_of_parts_normal, model_of_parts_cifra]
          spisok5 = [marka_of_parts + ' ' + model_of_parts_cifra + ' ' +diffferent_name]+ [marka_of_parts_normal, model_of_parts_cifra]

        spisok3 = [diffferent_name  + ' ' + model_of_parts]+ [marka_of_parts_normal, model_of_parts_normal]
        spisok4 = [marka_of_parts + ' ' + model_of_parts + ' ' + diffferent_name ]+ [marka_of_parts_normal, model_of_parts_normal]
        spisok6 = [model_of_parts + ' ' + diffferent_name ]+ [marka_of_parts_normal, model_of_parts_normal]

        if longer:
          spisok_zaprosov.extend([spisok1,spisok2,spisok3])
        else:
          spisok_zaprosov.extend([spisok6,spisok2, spisok5,spisok1 ])#spisok2, spisok5
  # вот тут проыерить ту ли он модель береттт...для записи в таргет
  #он наверное для всех моделей берет человеческую запись
  df = pd.DataFrame(spisok_zaprosov , columns = ['name', 'marka', 'model'])
  df['marka_model'] = df['marka'] +'/'+ df['model']
  return df
#df.to_csv('/content/drive/MyDrive/учеба/ВЫШКА/ДИПЛОМ/Example/name_marka_model_400000.csv')

def filtr_baza(df,  counts =25):
  '''
  '''
  mm = df.groupby('model')['name'].count() >counts
  aa = mm.loc[mm].index.to_list()
  df = df.query('model in @aa').copy()
  return df

import random

def zapros_example_graph1(df, counts=5):
  '''
  Формируем пользовательский запрос 
  counts - количество разделений одного запроса
  для обучения марка-модель
  формируем запрос исходя из того, что 50% выборки будет состоять из марки и полной модели
  30 % - из марки и частичной модели
  20% - только из модели
  '''
  spisok_zaprosov =[]
  for row in df:
    for i in range(counts):
      #нам нужен разномный выбор чтобы разнообразить наше описание товара
      random_choice = random.random()

      # входные данные
      name_of_parts = [row[0]]
      marka_of_parts = [row[1]] 
      model_of_parts = [row[2]]  

      random_name_parts = random.choices(spare_parts_name[row[0]])
      
      random_name_marka = random.choices(marka_mame[row[1]])

      #срез из словаря марок-моделей с пользовательском описании
      sr = baza_name_marke_model.loc[baza_name_marke_model.model_normal == row[2]].reset_index(drop=True).copy()
      # рандомно выбираем модель
      random_model = sr.model.tolist()
      # сюда пишем номер (конкретное число) из модели
      random_model_munber = str(int(sr.cifra[0]))

      if len(random_model) ==0:
        print('В базе не хватает модели - ', row[2])

      #выбираем рандомную модель из словаря
      random_name_model  = random.choices(random_model)

      #print(random_choice)
      # разделение на 2 ветки в зависимости от вероятности
      if random_choice > 0.5:
        spisok = (' '.join(random_name_parts +  random_name_marka + [row[2]]))
        spisok_zaprosov.append([spisok] + [row[0]] + [row[1]]+ [row[2]])
      elif 0.2 < random_choice < 0.5:
        spisok = (' '.join(random_name_parts +random_name_marka +  [random_model_munber] ))
        spisok_zaprosov.append([spisok] + [row[0]] + [row[1]] +[random_model_munber])
      else:
        spisok = (' '.join( random_name_parts + [row[2]]))
        spisok_zaprosov.append([spisok] +  [row[0]] + [row[1]]+ [row[2]])

  return spisok_zaprosov

def func_dly_avito(df, stolb,stolb2):
  '''
  Функция возвращает номера / или модели - из названия и описания в виде строки без дублей
  '''
  df['len'] = df[stolb2].apply(lambda x: len(x))
  df['oll'] =np.where(df['len']<4, df[stolb2]+ df[stolb] ,df[stolb])
  return df['oll'].apply(lambda x: ", ".join(str(i) for i in list(set(x))))

def create_triplet_df(df):
  '''
  Ввод:
  df : таблица

  В нашем примере данные для каждого товара представленны в виде списка:
  Запрос - Название товара - Марка - Модель - Номер - Общий номер(который включает в себя все одинаковые товары)

  Для Knowledge Graph мы составили структуру:
  Запрос- Отношение(Название товара) - Название
  Номер - Отношение(Название товара) - Название


  Например:

  head                                relation    tail
  K1006550	                          Название	  Насос основной
  Huyndai r180lc-7 механизм поворота	Марка	      Huyndai

  '''


  #примеры с номерами
  triplet_example_number = [[row[4], 'Общий_номер', row[5]]for row in df] #+\
                          #[[row[4], 'Название', row[1]] for row in df] +\
                          #[[row[4], 'Марка', row[2]]for row in df]+\
                          #[[row[4], 'Модель', row[3]]for row in df]
          

  #примеры с запросами        
  triplet_example_zapros = [[row[0], 'Название', row[1]] for row in df] +\
          [[row[0], 'Марка', row[2]]for row in df]+\
          [[row[0], 'Модель', row[3]]for row in df]+\
          [[row[0], 'Общий_номер', row[5]]for row in df]
  
  answer = pd.DataFrame(triplet_example_number + triplet_example_zapros, columns = ["head", "relation", "tail"]).drop_duplicates(ignore_index= True)

  return answer

import random

def zapros_example(df, counts=5):
  '''
  Формируем пользовательский запрос 
  counts - количество разделений одного запроса
  '''
  spisok_zaprosov =[]
  for row in df:
    for i in range(counts):
      #нам нужен разномный выбор чтобы разнообразить наше описание товара
      random_choice = random.random()

      random_name_parts = random.choices(spare_parts_name[row[0]])
      random_name_marka = random.choices(marka_mame[row[1]])

      # рандомно выбираем модель
      random_model = baza_name_marke_model.loc[baza_name_marke_model.model_normal == row[2]].model.tolist()

      if len(random_model) ==0:
        print('В базе не хватает модели - ', row[2])
      random_name_model  = random.choices(random_model)

      
      # разделение на 2 ветки в зависимости от вероятности
      if random_choice > 0.2:
        if random_choice > 0.6:
          spisok = (' '.join(random_name_parts + random_name_marka+ random_name_model + [row[3]]))
        else:
          spisok = (' '.join(random_name_parts + random_name_marka+ random_name_model ))
      else:
        if random_choice > 0.6:
          spisok = (' '.join(random_name_marka+ random_name_model + random_name_parts + [row[3]]))
        else:
          spisok = (' '.join(random_name_marka+ random_name_model + random_name_parts ))

      print([spisok] + row, [spisok], row)
      spisok_zaprosov.append([spisok] + row)
  return spisok_zaprosov

def prerare_exel(df,name = 0,group = 1, marka = 3, model = 2, number = 3):
  '''
  Подготовка датасета под стандарты - одна строка - один запрос
  Разбиаеться если много номеров или моделей
  '''
  answer = []
  for i, row in df.iterrows():
    for a in row[model].split(','):
      try:
        for b in row[number].split(','):
          if len(row) ==3:
            answer.append([row[name],a.strip(), b.strip(),])
          elif len(row) ==5:
            answer.append([row[name],row[group],row[marka],a.strip(), b.strip(),])
          elif len(row) >5:
            answer.append([row[0],row[1], a.strip(), b.strip(), row[4], row[5]])
          else:
            answer.append([row[0],row[1], a.strip(), b.strip(), row[4]])
      except:
          answer.append([row[name],row[1],a.strip()])



  return answer

def number_found(text):
  # регулярка для номеров ХЕндай Дусан Вольво

  reg_z = "[ \t\v\r\n\f]\d{2}[a-z,A-Z]{1}\d{1}[ ,-,‑,-]?\w{5}|\d{2}[n,q,N,Q,N]\w{1}[ ,-,‑,-]?\w{5}|\d{8}|[k,K,к,К]\d{1}[v,V]\w*|\d{2,6}-\d{3,5}|[K,k]?\d{7}|[X,x][a-z,A-Z]{3}[-,‑,-]?\d{5}|\d{3}-\d{2}-\d{3,5}|[ \t\v\r\n\f]\d{2}[a-z,A-Z]{2}[ ,-,‑,-]?\w{5}|[ \t\v\r\n\f][a-z,A-Z,А-Я,а-я]{4}[-,‑,-,-]?\d{5}[ \t\v\r\n\f]|[ \t\v\r\n\f]\d{3}[-,‑,-]?\d{5}"
  r =re.compile(reg_z)
  return [i.lower().strip() for i in re.findall(r,text)]

def model_found(text):
  # регулярка для номеров ХЕндай Дусан Вольво
  reg_z = "[ \t\v\r\n\f]\d{3}[ ,-]?[n,N]?[Л,l, L,л,l][c,C,с,С,c][ ,-]?[7,9]?|[ \t\v\r\n\f]\d{3}[ \$\f\n\r]|[ \t\v\r\n\f][r,R]\d{3}[ ,-]?[w,s,W, ,l, L,л]?[c,C,с,С]?[ ,-]?[5,6,7,9]?[\w][ \t\v\r\n\f]?|[E,E,e,е][c,с,С,C][, ]?\d{3}[\w]?|[b,B,б,Б][l,L,л,Л][, ]?\d{2}[\w,\w]?|[S,s,o,l,a,r]{5}[, ]?\d{3}[, ]?[,l, L,л]?[c,C,с,С]?[-]?\w?|[D,d,Д,д, ][x,X,х,Х,S,s]\d{3}[, ]?[,l, L,л]?[c,C,с,С]?[-]?\w?|[ \t\v\r\n\f]\d{3}[L,C,l,c]{2}[ ,-]?[V,v,A,a]|[r,R,р]\d{3}[ \t\v\r\n\f]|[ \t\v\r\n\f]\w{1}\d{3}"
  r =re.compile(reg_z)
  return [i.lower().strip() for i in re.findall(r,text)]

import re
def cifra(text):
  
  r =re.compile("\d{3,4}")
  russian =re.findall(r,text)
  return russian

baza_name_marke_model = pd.read_csv('/content/drive/MyDrive/учеба/ВЫШКА/ДИПЛОМ/Example/all_frame_model.csv' ,index_col=False).drop('Unnamed: 0', axis =1)
#tare_me_oll_parts(spare_parts_name, baza_name_marke_model)

"""# Word2Vec

Применение различных моделей Word2Vec (Word2Vec, FastText, предварительно обученная модель GloVe) к блоку текстов, указанному в качестве входных данных.
Создание словаря с внедрением слов.
"""

class_names, df = prepare_df_func(10, # количество групп
                     model_do_you_need=False,
                     oll_model_do_you_need =[210, 250, 225,170,180],# цифры моделей
                     count_out_model = 100 # количество исходных классов
                     )

df.marka_model.value_counts()

from gensim.models.phrases import Phrases, Phraser

#Создайте набор часто встречающихся слов
stoplist = set('цена купить'.split(' '))

# Строчные буквы каждого документа, разделить его пробелами и отфильтровать игнорируемые слова
sent = [[word for word in document.split() if word not in stoplist]
         for document in df["name"]]

phrases = Phrases(sent, min_count=3, progress_per=10000)

# все уникальные слова
from collections import Counter
token_counts = Counter([i for setn in sent for i in setn])
print("Total unique tokens :", len(token_counts))

# Добавьте специальные жетоны для неизвестных и пустых слов
UNK, PAD = "UNK", "PAD"
tokens = [UNK, PAD] + sorted(token_counts)
print("Vocabulary size:", len(tokens))

# присваиваем каждому слову - число
token_to_id = {}
for i, token in enumerate(tokens):
    token_to_id[token] = i

#token_to_id

UNK_IX, PAD_IX = map(token_to_id.get, [UNK, PAD])
# функция которая представляем последовательность токенов как матрицу
def as_matrix(sequences, max_len=None):
    """ Convert a list of tokens into a matrix with padding """
        
    max_len = min(max(map(len, sequences)), max_len or float('inf'))
    
    matrix = np.full((len(sequences), max_len), np.int32(PAD_IX))
    for i,seq in enumerate(sequences):
        row_ix = [token_to_id.get(word, UNK_IX) for word in seq[:max_len]]
        matrix[i, :len(row_ix)] = row_ix
    
    return matrix

def make_batch(sentences, targets, max_len=None, word_dropout=0):
    """
    Creates a keras-friendly dict from the batch data.
    """
    batch = {}
    batch["text"] = as_matrix(sentences, max_len)
    batch['target'] = np.array(targets)
    
    return batch

count_class = len(df.marka_model.value_counts())
RANDOM_SEED = 42
train_sentences, test_sentences,train_targets,test_targets = train_test_split(sent,df['marka_model_bert'] ,  test_size=0.3, random_state=RANDOM_SEED)

# proverka 
make_batch(train_sentences[10:20], train_targets[:10], max_len=10)

import torch
from torch import nn

class ConvNet(nn.Module):
    def __init__(self,number_count_,n_tokens=len(tokens), emb_size=20, 
                 kernel_sizes=[2,3,4],):
        super().__init__()
        # создаем эмбеддинги собственные
        self.embeddings = torch.nn.Embedding(n_tokens,emb_size)

        #или берта
        #self.enconder = bertmodel
        #создаем сверточные слои
        convs = [nn.Conv1d(in_channels = emb_size, out_channels = 100, 
                           kernel_size = kernel_size)
                         for kernel_size in kernel_sizes] 
                         
        self.conv_modules = nn.ModuleList(convs) #лист модулей по которым можно делать цикл
        # и в этом цикли ты будешь применять одни и теже операции для каждой свертки
        # и добавлять их в лист фичей feature_list
        self.drop = nn.Dropout()
        self.linear = nn.Linear(3*100,number_count_) # линейный слой, 100 выходных каналов у каждого свертоного слоя, и мы их будем конкретенировать
        self.softmax = nn.Softmax()

    def forward(self,batch):
        embeddings = self.embeddings(torch.LongTensor(batch['text']))
        embeddings = embeddings.transpose(1,2) # (batch_size, wordvec_size, sentence_length)
        
        feature_list = []
        for conv in self.conv_modules:
          feature_map = torch.nn.functional.relu(conv(embeddings))
          max_pooled , argmax = feature_map.max(dim = 2)
          feature_list.append(max_pooled)

        features = torch.cat(feature_list, dim=1) #конкретенируем фичи
        features = self.drop(features)
        linear = self.linear(features)
        return linear
    
    def predict(self, batch):
        return self.softmax(self.forward(batch))

model = ConvNet(count_class)

# proverka
#model.forward(make_batch(train_sentences[:10], train_targets[:10], max_len=10))

optimizer = torch.optim.Adam(model.parameters())
loss_fn = nn.CrossEntropyLoss()

from tqdm import tqdm_notebook
import random
from torch.autograd import Variable

#параметры модели
batch_size=60
dataset_arange = np.arange(len(train_sentences))
num_iters = 10000


train_sentences = np.array(train_sentences)
train_targets = np.array(train_targets)
test_batch = make_batch(test_sentences, test_targets, max_len=10)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# losses_train = []
# losses_test = []
# model.train()
# correct_predictions = 0
# history = defaultdict(list)
# 
# for i in tqdm_notebook(range(num_iters)):
#     
#     optimizer.zero_grad()
#     index = np.random.choice(dataset_arange, size=batch_size)
#     batch = make_batch(train_sentences[index], train_targets[index], max_len=10)
#     output = model.forward(batch)
#     targets = Variable(torch.LongTensor(batch['target']))    
#     loss = loss_fn(output, targets)
# 
#     loss.backward()
#     optimizer.step()
# 
#     if (i+10) % 100 == 0:
# 
#         # предсазания
# 
#         _, preds = torch.max(output, dim=1)
#         correct_predictions += torch.sum(preds == targets)
#         history['train_acc'].append(correct_predictions.double() / batch_size)
#         history['train_loss'].append(float(loss))
# 
#         output = model.forward(test_batch)
#         _, preds = torch.max(output, dim=1)
#         targets = Variable(torch.LongTensor(test_batch['target']))
#         correct_predictions += torch.sum(preds == targets)
#         history['val_acc'].append(correct_predictions.double() / len(test_batch['target']))
#         history['val_loss'].append(float(loss_fn(output, Variable(torch.LongTensor(test_batch['target'])))))
# 
# 
#         losses_train.append(float(loss))
#         losses_test.append(float(loss_fn(output, Variable(torch.LongTensor(test_batch['target'])))))
# 
#     if (i+10) % 500 == 0:
#         print("Train loss: ", losses_train[-1])
#         print("Test loss: ", losses_test[-1])

# график обучения
image_graph(history)

#torch.save(model, '/content/drive/MyDrive/учеба/ВЫШКА/ДИПЛОМ/Example/model_emmbed')

def fixpath(path):
  if __name__ == '__main__':
    return path
  return path.replace('/content/drive/MyDrive/учеба/ВЫШКА/ДИПЛОМ/Example/', 'data/')
  
model = torch.load(fixpath('/content/drive/MyDrive/учеба/ВЫШКА/ДИПЛОМ/Example/model_emmbed'))

model.eval()
#model.predict(make_batch(test_sentences[:10], test_targets[:10], max_len=10))

#new_dict_name = {value: key for key, value in enumerate(df[df_target].unique())}
class_names
cm = confusion_matrix(test_targets, predictions, labels=list(class_names.values()))
df_cm = pd.DataFrame(cm, index=class_names, columns=class_names)

df_cm.to_csv('/content/drive/MyDrive/учеба/ВЫШКА/ДИПЛОМ/Example/df_matrix_model_400000.csv')

df_cm

from sklearn.metrics import accuracy_score

predictions = model.predict(make_batch(test_sentences, test_targets, max_len=10))
predictions = np.argmax(predictions.detach().numpy(), axis=1)

name = 'marka_model_bert'
confusion_matrix_print(df, name,test_targets, predictions)

from sklearn.metrics import accuracy_score
accuracy_score(test_targets, predictions)

review_text = "210lc гидронасос насос"
def predict_marka_model(review_text,class_names, max_len):
  '''
  review_text - запрос
  class_names - словарь название модели - таргет берта
  max_len - максимальная длиннна запроса
  '''
  sent = [word for word in review_text.split() if word not in stoplist]
  row_ix = [token_to_id.get(word, UNK_IX) for word in sent[:max_len]]
  matrix = np.full((1, max_len), np.int32(PAD_IX))
  matrix[0, :len(row_ix)] = row_ix
  pr = np.argmax(model.predict({"text" : matrix}).detach().numpy(), axis=1)[0]
  ix2word = dict(enumerate(class_names))
  return ix2word[pr]

predict_marka_model(review_text, class_names,max_len)

class_names

proverka = pd.read_csv('/content/drive/MyDrive/учеба/ВЫШКА/ДИПЛОМ/Example/proverka.csv' )#.fillna() #,index_col=False)#.drop('Unnamed: 0', axis =1)

proverka = proverka.dropna(subset=['name'])

proverka['predict'] = proverka['name'].apply(lambda x: predict_marka_model(x,class_names,10 ))

proverka['marka_model'] = proverka['marka'] +'/' +proverka['model']

accuracy_score(proverka['marka_model'].to_list(), proverka['predict'].to_list())

proverka.to_csv('/content/drive/MyDrive/учеба/ВЫШКА/ДИПЛОМ/Example/proverka_end.csv')

eee

"""# BERT"""

#!pip install -qq transformers

from transformers import BertModel,BertPreTrainedModel
import torch.nn as nn
#from model.resnet import ResModel
import torch
import torch.nn.functional as F

class bertmodel(BertPreTrainedModel):
    def __init__(self,config):
        super(bertmodel, self).__init__(config)
        self.bert = BertModel(config)
        self.dropout = nn.Dropout(0.2)

    def forward(self,qk_input_ids,qk_attention_mask):
        output = self.bert(qk_input_ids,qk_attention_mask)
        # cls = torch.mul(qk_attention_mask.unsqueeze(-1),output[0])
        cls = self.dropout(output[0])
        return cls
        
class model_xq_model(nn.Module):
    def __init__(self,bertmodel):
        super(model, self).__init__()
        self.enconder = bertmodel
        self.c_embed = nn.Embedding(3,1024)
        self.conv2d = nn.Conv2d(1, 3, (2 * 5 + 1, 3), padding=(5, 0))
        # self.res = ResModel()
        self.dense = nn.Linear(9*1024,3)
        self.pool = nn.AdaptiveMaxPool2d((9,1024))
        self.dropout = nn.Dropout(0.2)
        self.relu = nn.GELU()

    def forward(self,qk_input_ids,qk_attention_mask):
        batch_size = qk_attention_mask.shape[0]
        labels = self.c_embed.weight.data
        c = labels.transpose(-2,-1)
        c = F.normalize(c,p=2,dim=0)
        cls = self.enconder(qk_input_ids,qk_attention_mask)
        cls1 = torch.mul(qk_attention_mask.unsqueeze(-1), cls)
        cls1 = F.normalize(cls1,p=2,dim=2)
        g = torch.matmul(cls1,c)
        g = g.unsqueeze(1)
        u = F.relu(self.conv2d(g))
        umax,_ = torch.max(u.squeeze(-1), dim=1)
        attention_score = torch.softmax(umax,dim=-1)
        attention_score = attention_score.unsqueeze(-1)
        z = torch.mul(cls, attention_score)
        # out = self.res(z.unsqueeze(1))
        pred = self.dense(self.dropout(self.relu(self.pool(z).reshape(batch_size,-1))))

        return pred

import argparse

def getparse():
    parse = argparse.ArgumentParser()
    parse.add_argument('--lstm_hidden',default=512,type=int,help='lstm_hidden')
    parse.add_argument('--lstm_num_layers',default=2,type=int,help='lstm_layers')
    parse.add_argument("--GPUNUM",default = 0,type=int)
    parse.add_argument("--data_path",default='data/Xeon3NLP_round1_train_20210524.txt',type=str)
    parse.add_argument('--num_train',default=18000,type=int)
    parse.add_argument("--bert_path",default='bert',type=str)
    parse.add_argument('--max_length',default=50,type=int)
    parse.add_argument('--hidden',default=1024,type=int,help='bert_hidden')
    parse.add_argument("--lr",default = 8e-6,help='learning_rate' )
    parse.add_argument('--batch_size',default=32,type=int)
    parse.add_argument('--test_path',default='data/Xeon3NLP_round1_test_20210524.txt')

    return parse

'''
bert_model = bertmodel.from_pretrained('bert-base-cased')
#bert_model = bertmodel.from_pretrained('bert-base-cased').to(device)
#args = getparse().parse_args()
xq_model = model_xq_model(bert_model).to(device)
'''

"""## 2"""

class ConvNet(nn.Module):
    def __init__(self,number_count_,n_tokens=len(tokens), emb_size=20, 
                 kernel_sizes=[2,3,4],):
        super().__init__()
        # создаем эмбеддинги собственные
        self.embeddings = torch.nn.Embedding(n_tokens,emb_size)

        #или берта
        self.enconder = bertmodel
        #создаем сверточные слои
        convs = [nn.Conv1d(in_channels = emb_size, out_channels = 100, 
                           kernel_size = kernel_size)
                         for kernel_size in kernel_sizes] 
                         
        self.conv_modules = nn.ModuleList(convs) #лист модулей по которым можно делать цикл
        # и в этом цикли ты будешь применять одни и теже операции для каждой свертки
        # и добавлять их в лист фичей feature_list
        self.drop = nn.Dropout()
        self.linear = nn.Linear(3*100,number_count_) # линейный слой, 100 выходных каналов у каждого свертоного слоя, и мы их будем конкретенировать
        self.softmax = nn.Softmax()

    def forward(self,batch):
        embeddings = self.embeddings(torch.LongTensor(batch['text']))
        embeddings = embeddings.transpose(1,2) # (batch_size, wordvec_size, sentence_length)
        
        feature_list = []
        for conv in self.conv_modules:
          feature_map = torch.nn.functional.relu(conv(embeddings))
          max_pooled , argmax = feature_map.max(dim = 2)
          feature_list.append(max_pooled)

        features = torch.cat(feature_list, dim=1) #конкретенируем фичи
        features = self.drop(features)
        linear = self.linear(features)
        return linear
    
    def predict(self, batch):
        return self.softmax(self.forward(batch))

"""### Функции"""

import transformers
from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup

PRE_TRAINED_MODEL_NAME = 'bert-base-cased'
tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)

RANDOM_SEED = 42
np.random.seed(RANDOM_SEED)
torch.manual_seed(RANDOM_SEED)
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

class SentimentClassifier(nn.Module):

  def __init__(self, n_classes):
    super(SentimentClassifier, self).__init__()
    self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)
    self.drop = nn.Dropout(p=0.3)
    self.out = nn.Linear(self.bert.config.hidden_size, n_classes)
  
  def forward(self, input_ids, attention_mask):
    outputs = self.bert(
      input_ids=input_ids,
      attention_mask=attention_mask
    )
    output = self.drop(outputs["pooler_output"])
    return self.out(output)

MAX_LEN = 25

import torch
from torch import nn, optim
from torch.utils.data import Dataset, DataLoader

class GPReviewDataset(Dataset):

  def __init__(self, reviews, targets, tokenizer, max_len):
    self.reviews = reviews
    self.targets = targets
    self.tokenizer = tokenizer
    self.max_len = max_len
  
  def __len__(self):
    return len(self.reviews)
  
  def __getitem__(self, item):
    review = str(self.reviews[item])
    target = self.targets[item]

    encoding = self.tokenizer.encode_plus(
      review,
      add_special_tokens=True,
      max_length=self.max_len,
      return_token_type_ids=False,
      pad_to_max_length=True,
      truncation=True,
      return_attention_mask=True,
      return_tensors='pt',
    )

    return {
      'review_text': review,
      'input_ids': encoding['input_ids'].flatten(),
      'attention_mask': encoding['attention_mask'].flatten(),
      'targets': torch.tensor(target, dtype=torch.long)
    }


def create_data_loader(df, name_target, tokenizer, max_len, batch_size):
  ds = GPReviewDataset(
    reviews=df.name.to_numpy(),
    targets=df[name_target].to_numpy(),

    tokenizer=tokenizer,
    max_len=max_len
  )

  return DataLoader(
    ds,
    batch_size=batch_size,
    num_workers=4
  )

def eval_model(model, data_loader, loss_fn, device, n_examples):
  model = model.eval()

  losses = []
  correct_predictions = 0

  with torch.no_grad():
    for d in data_loader:
      input_ids = d["input_ids"].to(device)
      attention_mask = d["attention_mask"].to(device)
      targets = d["targets"].to(device)

      outputs = model(
        input_ids=input_ids,
        attention_mask=attention_mask
      )
      _, preds = torch.max(outputs, dim=1)

      loss = loss_fn(outputs, targets)

      correct_predictions += torch.sum(preds == targets)
      losses.append(loss.item())

  return correct_predictions.double() / n_examples, np.mean(losses)

def get_predictions(model, data_loader):
  model = model.eval()
  
  review_texts = []
  predictions = []
  prediction_probs = []
  real_values = []

  with torch.no_grad():
    for d in data_loader:

      texts = d["review_text"]
      input_ids = d["input_ids"].to(device)
      attention_mask = d["attention_mask"].to(device)
      targets = d["targets"].to(device)

      outputs = model(
        input_ids=input_ids,
        attention_mask=attention_mask
      )
      _, preds = torch.max(outputs, dim=1)

      probs = F.softmax(outputs, dim=1)

      review_texts.extend(texts)
      predictions.extend(preds)
      prediction_probs.extend(probs)
      real_values.extend(targets)

  predictions = torch.stack(predictions).cpu()
  prediction_probs = torch.stack(prediction_probs).cpu()
  real_values = torch.stack(real_values).cpu()
  return review_texts, predictions, prediction_probs, real_values

def train_epoch(
    
  model, 
  data_loader, 
  loss_fn, 
  optimizer, 
  device, 
  scheduler, 
  n_examples
 ):
  model = model.train()

  losses = []
  correct_predictions = 0
  
  for d in data_loader:
    input_ids = d["input_ids"].to(device)
    attention_mask = d["attention_mask"].to(device)
    targets = d["targets"].to(device)

    outputs = model(
      input_ids=input_ids,
      attention_mask=attention_mask
    )
    ################################################
    #print(outputs.shape) #смотрим выходы слоя
    #print(outputs)
    _, preds = torch.max(outputs, dim=1)
    loss = loss_fn(outputs, targets)

    correct_predictions += torch.sum(preds == targets)
    losses.append(loss.item())

    loss.backward()
    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
    optimizer.step()
    scheduler.step()
    optimizer.zero_grad()

  return correct_predictions.double() / n_examples, np.mean(losses)

def model_trein(df, model, df_target,EPOCHS, BATCH_SIZE,RANDOM_SEED = 42):
  #%%time
  #df_target = 'name_bert' #'marka_bert' # 'model_bert'  # 
  #EPOCHS = 15 # для марки номр, для модели запустила дважды 3 для имени

  df_train, df_test = train_test_split(df, test_size=0.3, random_state=RANDOM_SEED)
  df_val, df_test = train_test_split(df_test, test_size=0.5, random_state=RANDOM_SEED)

  df_target = str(df_target)+'_bert' 
  train_data_loader = create_data_loader(df_train,df_target, tokenizer, MAX_LEN, BATCH_SIZE)
  val_data_loader = create_data_loader(df_val,df_target, tokenizer, MAX_LEN, BATCH_SIZE)
  test_data_loader = create_data_loader(df_test,df_target, tokenizer, MAX_LEN, BATCH_SIZE)

  optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)
  total_steps = len(train_data_loader) * EPOCHS

  scheduler = get_linear_schedule_with_warmup(
    optimizer,
    num_warmup_steps=0,
    num_training_steps=total_steps
  )
  loss_fn = nn.CrossEntropyLoss().to(device) #

  #

  history = defaultdict(list)
  best_accuracy = 0

  for epoch in range(EPOCHS):

    print(f'Epoch {epoch + 1}/{EPOCHS}')
    print('-' * 10)

    train_acc, train_loss = train_epoch(
      model,
      train_data_loader,    
      loss_fn, 
      optimizer, 
      device, 
      scheduler, 
      len(df_train)
    )

    print(f'Train loss {train_loss} accuracy {train_acc}')

    val_acc, val_loss = eval_model(
      model,
      val_data_loader,
      loss_fn, 
      device, 
      len(df_val)
    )

    print(f'Val   loss {val_loss} accuracy {val_acc}')
    print()

    history['train_acc'].append(train_acc)
    history['train_loss'].append(train_loss)
    history['val_acc'].append(val_acc)
    history['val_loss'].append(val_loss)

    if val_acc > best_accuracy:
      torch.save(model.state_dict(), 'best_model_state.bin')
      best_accuracy = val_acc

  return model, history, test_data_loader

"""### marka

#### Обучение
"""

df = prepare_df_func(1,oll_model_do_you_need =[210, 250, 225], )

df.marka_model.value_counts()

from transformers import logging

logging.set_verbosity_warning()#BertForSequenceClassification

# Commented out IPython magic to ensure Python compatibility.
# '''
# %%time
# df_target = 'marka_model'
# EPOCHS = 100
# BATCH_SIZE = 16
# 
# # словарь из 
# class_names = {value: key for key, value in enumerate(df[df_target].unique())}
# df[str(df_target) +'_bert'] = df[df_target].map(class_names)
# 
# model_marka_model = SentimentClassifier(len(class_names)).to(device) 
# model_marka_model, history, test_data_loader = model_trein(df, model_marka_model ,df_target,EPOCHS, BATCH_SIZE)
# 
# torch.save(model_marka_model, '/content/drive/MyDrive/учеба/ВЫШКА/ДИПЛОМ/Example/model_marka_model')
# #model_marka = torch.load('/content/drive/MyDrive/учеба/ВЫШКА/ДИПЛОМ/Example/model_marka_model')
# '''

'''
# график обучения
image_graph(history)

torch.save(model_marka_model, '/content/drive/MyDrive/учеба/ВЫШКА/ДИПЛОМ/Example/model_marka_model_400_only_name')
'''

'''
import torch.nn.functional as F

y_review_texts, y_pred, y_pred_probs, y_test = get_predictions(
  model_marka_model, test_data_loader)

confusion_matrix_print(df, df_target,y_test, y_pred)


review_text = "210lc-7 гидронасос"

MAX_LEN = 16
encoded_review = tokenizer.encode_plus(
  review_text,
  max_length=MAX_LEN,
  add_special_tokens=True,
  return_token_type_ids=False,
  pad_to_max_length=True,
  return_attention_mask=True,
  return_tensors='pt',
)

input_ids = encoded_review['input_ids'].to(device)
attention_mask = encoded_review['attention_mask'].to(device)
ix2word = dict(enumerate(class_names))
output = model_marka_model(input_ids, attention_mask)
_, prediction = torch.max(output, dim=1)

print(f'Review text: {review_text}')
print(f'Sentiment  : {ix2word[prediction.item()]}')
'''

'''
new_dict_name = {value: key for key, value in enumerate(df[df_target].unique())}
cm = confusion_matrix(y_test, y_pred, labels=list(new_dict_name.values()))
df_cm = pd.DataFrame(cm, index=new_dict_name, columns=new_dict_name)

df_cm.to_csv('/content/drive/MyDrive/учеба/ВЫШКА/ДИПЛОМ/Example/df_matrix_model_400000.csv')
'''

"""### name"""

'''
df = pd.read_csv('/content/drive/MyDrive/учеба/ВЫШКА/ДИПЛОМ/Example/df_for_train_bert.csv')
df[:3]

df_target = 'name'
EPOCHS = 3
BATCH_SIZE = 32

# словарь из df_target
class_names = {value: key for key, value in enumerate(df[df_target].unique())}
df[str(df_target) +'_bert'] = df[df_target].map(class_names)

model_name = SentimentClassifier(len(class_names)).to(device) 

model_name, history, test_data_loader = model_trein(df, model_name ,df_target,EPOCHS, BATCH_SIZE)

torch.save(model_name, '/content/drive/MyDrive/учеба/ВЫШКА/ДИПЛОМ/Example/model_name')
#model_name = torch.load('/content/drive/MyDrive/учеба/ВЫШКА/ДИПЛОМ/Example/model_name')
'''

'''
# график обучения
image_graph(history)
import torch.nn.functional as F

y_review_texts, y_pred, y_pred_probs, y_test = get_predictions(
  model, test_data_loader)

confusion_matrix_print(df, df_target)
'''

"""!!! посмотреть как берт обучаеться на запросах без марки

### model
"""

import re
import numpy as np
import pandas as pd

'''
df = pd.read_csv('/content/drive/MyDrive/учеба/ВЫШКА/ДИПЛОМ/Example/name_marka_model.csv')
df.columns=['name','marka','model']
len(df)
'''

''''
df_target = 'model'
EPOCHS = 20
BATCH_SIZE = 16

# словарь из 
class_names = {value: key for key, value in enumerate(df[df_target].unique())}
df[str(df_target) +'_bert'] = df[df_target].map(class_names)

#model_model = SentimentClassifier(len(class_names)).to(device) 

model_model, history, test_data_loader = model_trein(df, model_model ,df_target,EPOCHS, BATCH_SIZE)

torch.save(model_model, '/content/drive/MyDrive/учеба/ВЫШКА/ДИПЛОМ/Example/model_model')
#model_model = torch.load('/content/drive/MyDrive/учеба/ВЫШКА/ДИПЛОМ/Example/model_model')
'''

#torch.save(model_model, '/content/drive/MyDrive/учеба/ВЫШКА/ДИПЛОМ/Example/model_model')

# график обучения 40 эпох батч 32 + 15 батч 64 + 20 батч 16
#image_graph(history)

# график обучения # 20 этох батч 64 + 40 эпох батч 128
#image_graph(history)

'''
import torch.nn.functional as F

y_review_texts, y_pred, y_pred_probs, y_test = get_predictions(
  model_model, test_data_loader)

confusion_matrix_print(df, df_target)
'''

"""## общий номер

#Работа
"""

'''
#создаем рандомные пользовательский запрос - из наших данных - меняем написание запчасти, марки, модели
parts_mainpump_prerare = prerare_exel(parts_mainpump)

train = zapros_example(spare_parts_example) + zapros_example(parts_mainpump_prerare)
test= zapros_example(spare_parts_example_test)
len(train), len(test)
# данные для обучения
triplet_example_df = create_triplet_df(train)

# тестовые данные
triplet_example_df_test = create_triplet_df(test)
len(triplet_example_df), len(triplet_example_df_test)
'''

"""# Как мы работаем с запросом пользователя

### sqlite3
"""

import pandas as pd
import numpy as np
from collections import Counter
import re

# загружаем нашу базу из файла
baza = pd.DataFrame(prerare_exel(pd.read_csv('/content/drive/MyDrive/учеба/ВЫШКА/ДИПЛОМ/Example/parts_mainpump.csv' \
                      ,index_col=False)),columns=['name','marka','model','parts_number', 'oll_number', 'price'])

len(baza)

baza.query('parts_number == "401-00059"')

"""### Пример работы"""

import sqlite3

baza.to_sql('baza', sqlite3.connect('db1'), index = False, if_exists = 'replace')

import re
def exit_sql(zapros,type_of = 'price'):
  '''
  Функция ищет запрос в баще данных и возращает цену и дополнительные номера
  type_of может быть / model - получаем все модели
                    / number - получаем дополнительные номера
                    / price - цена
  '''
  with sqlite3.connect('db1') as db:
    cursor = db.cursor()
    # выделение номера
    try:
      value = list(map(number_found, [zapros.lower()]))[0][0].strip().lower()
    except:
      return print('Номер не найден - переходим к плану Б')
      # формирование запроса
    try:
      # вот сюда нужно втыкнуть чтобы он выводил все номера по этому общему номеру
      
      if type_of == 'model':
        sql =f'select DISTINCT b.marka, b.model from baza as b \
              where oll_number == (SELECT oll_number from baza where parts_number = "{value}")'
      elif type_of == 'number':
        sql =f'select DISTINCT b.parts_number from baza as b \
              where oll_number == (SELECT oll_number from baza where parts_number = "{value}")'
      else:
        sql =f'SELECT DISTINCT name,price from baza where parts_number = "{value}"'
      cursor.execute(sql)
      
      #ответ
      skins = cursor.fetchall()
      cursor.close()
      
      return skins#print((f"{skins[0][0]}. Цена - {skins[0][1]}р. "))
    except:
      return print('Номер не найден - переходим к плану Б')

exit_sql('31n7-40010', 'number')

exit_sql('31n7-40010', 'model')

exit_sql('зедуктор на экскаватор 31n7-40010')

"""План Б состоит в том. чтобы извлечь из строки название марку и модель - и произвести поиск по совпадениям в базе данных"""

#пока тут ломаем
